{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_5_ASSIGNMENTS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvPdg3g8YCc-"
      },
      "source": [
        "# Project 5: Transfer Learning and Feature Extraction\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHh0mY6XYTCT"
      },
      "source": [
        "## Assignments\n",
        "\n",
        "Please, edit your report by fulfilling the following list of assignments.\n",
        "\n",
        "**Introduction.** Short summary of the goals of the  project. The sections composing the report.\n",
        "\n",
        "**Section 1. Data loading and preparation**\n",
        "\n",
        "* 1a. Download and preprocess a sample image;\n",
        "* 2a. Show the sample image.\n",
        "\n",
        "**Section 2. The VGG16 model** \n",
        "* 1a. Download the net, pre-trained.\n",
        "\n",
        "**Section 3. Extracting features**\n",
        "* 3a. Visualize the learned kernels of the first convolutional layer;\n",
        "* 3b. Visualize the feature maps of the feature extraction block.\n",
        "\n",
        "**Results, observations and conclusions**\n",
        "\n",
        "**Full code**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8QIhbO1ZDAw"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZsKgt_cZZkD"
      },
      "source": [
        "## 1. Data loading and preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZkGd1ph9S9M"
      },
      "source": [
        "Import the libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbbf4KPzc-3y"
      },
      "source": [
        "import urllib.request\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGyGZVso9WHq"
      },
      "source": [
        "Define the device to use during the computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vT0yctfc9mV"
      },
      "source": [
        "## TODO: if available use GPU, else use CPU.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBDDQXqTZdk-"
      },
      "source": [
        "### 1a. Download and preprocess a sample image\n",
        "\n",
        "Download an example image from the pytorch website."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPPbpWOcZuk2"
      },
      "source": [
        "# Download a sample image\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBScH-xm9j8h"
      },
      "source": [
        "Open the image and apply a transform ([documentation](https://pytorch.org/vision/stable/transforms.html)):\n",
        "* `Resize` it to 256x256 pixels;\n",
        "* `CenterCrop` it to size 224;\n",
        "* transform it `ToTensor`;\n",
        "* `Normalize` it with `mean=[0.485, 0.456, 0.406]` and `std=[0.229, 0.224, 0.225]`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt_lVKCPcuRf"
      },
      "source": [
        "input_image = Image.open(filename)\n",
        "\n",
        "# TODO: complete the transform.\n",
        "transform = transforms.Compose([\n",
        "    # Resize to 256\n",
        "    # CenterCrop to 224\n",
        "    # ToTensor\n",
        "    # Normalize\n",
        "])\n",
        "\n",
        "# apply the transform\n",
        "image = transform(input_image)\n",
        "\n",
        "# create a mini-batch as expected by the model\n",
        "# by adding a dimension at position 0\n",
        "image = image.unsqueeze(0) \n",
        "\n",
        "## TODO: print the shape of the image.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-O0g1xgKFoA"
      },
      "source": [
        "### 2a. Show the sample image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvzKAWZogLhL"
      },
      "source": [
        "## TODO: visualize the normalized image.\n",
        "# hint: create a new variable (e.g. 'image_show') to store the modifications\n",
        "#       you have to make to show the image (squeeze, permute, clip...). By doing\n",
        "#       so, you will keep the original 'image' intact to \"feed\" it to the \n",
        "#       network in the next steps.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ktN27YNkRA3"
      },
      "source": [
        "## 2. The VGG16 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnBC2QKwkUv4"
      },
      "source": [
        "### 2a. Download the net, pre-trained \n",
        "\n",
        "Download the VGG16 network ([documentation](https://pytorch.org/hub/pytorch_vision_vgg/)), pre-trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mjkl2D2kf8L"
      },
      "source": [
        "## TODO: Load the VGG16 model and set it in evaluation mode.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp5qKw9HKuTz"
      },
      "source": [
        "## 3. Extracting features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR9dCXs2LBW_"
      },
      "source": [
        "### 3a. Visualize the learned kernels of the first convolutional layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmyS7G6RLAa9"
      },
      "source": [
        "## TODO: using the 'children' method, extract the weights of the 1st\n",
        "#        convolutional layer, and visualize all of them in a grid.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojBaIY5ULjVc"
      },
      "source": [
        "### 3b. Visualize the feature maps of the feature extraction block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD1M1PUWLwFF"
      },
      "source": [
        "#### Slice the network at the 1st and 3rd convolutional layer\n",
        "\n",
        "By accessing the `features` block of the network, we can \"slice\" it at a specific layer. Slice the network at the first and third convolutional layer, and add them to a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOq841yNlbdq"
      },
      "source": [
        "image.to(device)\n",
        "\n",
        "## TODO: slice the network at the 1st and 3rd convolutional layer.\n",
        "# example: to slice the network at the 2nd convolutional layer, look at the \n",
        "#          VGG16 structure. The second conv layer is at position (2):\n",
        "#               (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "#          To get a network that goes from the very first layer to the 2nd conv\n",
        "#          layer, and pass throug it an image, we can write\n",
        "#               model.features[:3](image) \n",
        "outputs = []\n",
        "outputs.extend([\n",
        "    model.features[#...](image),  #  1st conv layer\n",
        "    #...                          #  3rd conv layer\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcFpMiJ1Rpy_"
      },
      "source": [
        "#### Visualize 16 feature maps from each convolutional layer\n",
        "\n",
        "Visualize 16 feature maps from the first and the third convolutional layer. Note that each of the layers has more than 16 feature maps, but we want to show only 16 to visualize them better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niVK6It2rUKx"
      },
      "source": [
        "## TODO: visualize 16 feature maps (filters) from the 1st convolutional layer \n",
        "#        and 16 feature maps from the 3rd convolutional layer, each of them in \n",
        "#        a 4x4 grid.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeI7IQRYT3Fv"
      },
      "source": [
        "## Results, observations and conclusions\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN04kYw5T7Mo"
      },
      "source": [
        "## Full code\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqHTGxu-uCJb"
      },
      "source": [
        "---\n",
        "\n",
        "## Bibliography\n",
        "* [VGG-NETS](https://pytorch.org/hub/pytorch_vision_vgg/)\n",
        "* [Visualizing Filters and Feature Maps in Convolutional Neural Networks using PyTorch](https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/)"
      ]
    }
  ]
}