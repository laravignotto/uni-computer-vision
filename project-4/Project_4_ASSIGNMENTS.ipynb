{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_4_ASSIGNMENTS (2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpQRE7VmqS-c"
      },
      "source": [
        "# Project 4: Image Classification and Feature Extraction\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32s3G0_kqmzq"
      },
      "source": [
        "## Assignments\n",
        "\n",
        "Please, edit your report by fulfilling the following list of assignments.\n",
        "\n",
        "**Introduction.** Short summary of the goals of the  project. The sections composing the report.\n",
        "\n",
        "**Section 1. Data loading and preparation**\n",
        "\n",
        "Download, decompress, analyse:\n",
        "\n",
        "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
        "\n",
        "* 1a. Download the dataset;\n",
        "* 1b. Load the dataset and check its characteristics;\n",
        "* 1c. Show the first 4 images for each category;\n",
        "* 1d. Data preparation.\n",
        "\n",
        "**Section 2. The AlexNet model** \n",
        "* 2a. Download the net, not pre-trained.\n",
        "\n",
        "**Section 3. Training the Net**\n",
        "* 3a. Design the architecture;\n",
        "* 3b. Define the batch size and load the training and test set;\n",
        "* 3c. Define a training function;\n",
        "* 3d. Run the training;\n",
        "* 3e. Save the trained network.\n",
        "\n",
        "**Section 4. Test and performance evaluations**\n",
        "* 4a. Define and execute a test function;\n",
        "* 4b. Performance curves.\n",
        "\n",
        "**Section 5. Extracting features**\n",
        "* 5a. Load the saved network and set it in evaluation mode;\n",
        "* 5b. Visualize the learned kernels of the first convolutional layer;\n",
        "* 5c. Visualize the feature maps.\n",
        "\n",
        "**Results, Observations and Conclusions**\n",
        "Write your own notes, observations and conclusions about the results of your work.\n",
        "\n",
        "**Full Code**\n",
        "Report the complete code of the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpzsltE0roKJ"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This report is divided in ...TODO \n",
        "\n",
        "The aim of this project is ...TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNbD9iAurr3w"
      },
      "source": [
        "## 1. Data loading and preparation\n",
        "\n",
        "Import the libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUVq3Vciwje8"
      },
      "source": [
        "!pip install torch-inspect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r10wcYk9tfQ2"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 1a. Download the dataset\n",
        "from torchvision.datasets.utils import download_and_extract_archive\n",
        "\n",
        "# 2a. (show AlexNet structure)\n",
        "import torch_inspect as ti "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pV_MKHDxmh-"
      },
      "source": [
        "Define the device to use during the computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqwjwPcUxrAW"
      },
      "source": [
        "## TODO: if available use GPU, else use CPU\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cS-A3o3yEFH"
      },
      "source": [
        "### 1a. Download the dataset\n",
        "Description of the dataset: [tf_flowers](https://www.tensorflow.org/datasets/catalog/tf_flowers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1qVvRzOyESr"
      },
      "source": [
        "# download flower_photos  \n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\" #URL to download file from\n",
        "filename = \"flower_photos.tgz\" #name to save the file under\n",
        "root = \"~/tmp/\" #directory to place downloaded file in\n",
        "download_and_extract_archive(url, root, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jsWNtMRXblo"
      },
      "source": [
        "### 1b. Load the dataset and check its characteristics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5F_hrYEgT-2"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "## TODO: load the dataset into the 'data' variable\n",
        "# use ImageFolder -> https://pytorch.org/vision/stable/datasets.html#imagefolder\n",
        "# hints: \n",
        "#   * you only need 'root' and 'transform';\n",
        "#   * use 'flower_photos.tgz/flower_photos/' as root.\n",
        "data = #...\n",
        "\n",
        "## TODO: check the dataset size and get the names of the classes\n",
        "# hint: check the attributes of the ImageFolder class to get the classes' names\n",
        "# https://pytorch.org/vision/stable/_modules/torchvision/datasets/folder.html#ImageFolder\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McifJQDAioVY"
      },
      "source": [
        "#### Summarize the number of images for each category and display the summary in a table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogg-LYpztgsS"
      },
      "source": [
        "# calculate the number of samples in each class:\n",
        "#   * torch.unique returns the unique elements of the input tensor;\n",
        "#   * the .targets attrubute gets the class_index value for each image in the dataset.\n",
        "_, imgs_per_class = torch.unique(torch.tensor(data.targets), return_counts=True)\n",
        "\n",
        "## TODO: print the number of elements in each class\n",
        "# hint: print 'imgs_per_class' to inspect it\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFfB8JrWjOGJ"
      },
      "source": [
        "#### Print the resolution of the first image for each category\n",
        "\n",
        "Note that the samples are arranged per class, meaning that the first N images belong to class 0, the following M elements belong to the class 1, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZFNKDGNtibO"
      },
      "source": [
        "## TODO: print the resolution of the first image for each category\n",
        "\n",
        "# Note: there are many ways to do this, here's a suggestion:\n",
        "#   * get the number of the classes;\n",
        "#   * create a dictionary that maps the labels (integers) to the name of the class;\n",
        "#   * create a list with the indices of the first image of each class. You already have\n",
        "#     the number of elements in each class, you could incrementally add them to get the\n",
        "#     indices using 'accumulate' from 'itertools' ('from itertools import accumulate');\n",
        "#   * now you have all the data you need and can print the resolution of the \n",
        "#     first image for each category.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTjr4PJ9s0Cy"
      },
      "source": [
        "### 1c. Show the first 4 images for each category\n",
        "Print them in a grid with 4 columns (samples) and 5 rows (classes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtqN5E2WtjER"
      },
      "source": [
        "## TODO: show the first 4 images for each category\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcpwYXyrngO8"
      },
      "source": [
        "### 1d. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acrKACAGnkbn"
      },
      "source": [
        "#### Load and normalize the data\n",
        "\n",
        "Resize the images to 64x64 pixels and normalize the pixel values.\n",
        "\n",
        "To normalize ([documentation](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Normalize)) the tensor images, calculate the mean and standard deviation for the 3 colour channels of the images over the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBT4BOS5ite_"
      },
      "source": [
        "IMGS_DIM = 64\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((IMGS_DIM, IMGS_DIM)) #resize all images to IMGS_DIMxIMGS_DIM pixels\n",
        "])\n",
        "\n",
        "## TODO: load the dataset into the 'data' variable as before\n",
        "data = #...\n",
        "\n",
        "# stack all the images by iterating over the dataset and extracting the images\n",
        "imgs = torch.stack([img for img, _ in data], dim=3)\n",
        "print(imgs.shape)\n",
        "\n",
        "# keep 3 channels and merge all the remaining dimensions into one \n",
        "temp = imgs.view(3, -1)\n",
        "print(temp.shape)\n",
        "\n",
        "# calculate the mean over the elements of each channel\n",
        "mean = temp.mean(dim=1)\n",
        "\n",
        "# calculate the standard deviation over the elements of each channel\n",
        "std = temp.std(dim=1)\n",
        "\n",
        "print(mean, std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMHyeA975Dpq"
      },
      "source": [
        "Use these values to normalize our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY5CeF_joJpP"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std), #normalize the pixel values\n",
        "    transforms.Resize((IMGS_DIM, IMGS_DIM)) #resize all images to IMGS_DIMxIMGS_DIM pixels\n",
        "])\n",
        "\n",
        "## TODO: load the dataset into the 'data' variable as before\n",
        "data = #..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSLzIImbs485"
      },
      "source": [
        "#### Constrain to 70 the number of images per category. Check this by printing a table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMMpKGGptj00"
      },
      "source": [
        "## TODO: get all the first 70 indices for each class\n",
        "# put them in a list called 'indices'; use the list with the indices of \n",
        "# the first image of each class you created in 1a.\n",
        "\n",
        "\n",
        "# create a subset of the original dataset with just 70 samples per class\n",
        "data2 = torch.utils.data.Subset(data, indices)\n",
        "\n",
        "## TODO: \n",
        "#   * check if the size of the dataset is 70x5=350\n",
        "#   * check if each class contains 70 elements. Hint: you could use 'Counter'\n",
        "#     ('from collections import Counter')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19vrhEY9tAeA"
      },
      "source": [
        "#### Define the Training and Test sets (300/50) and randomize the images\n",
        "\n",
        "Define the size of the training set and test set, and construct them randomizing the images using `random_split`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8-zEN3KtlOr"
      },
      "source": [
        "train_size = 300\n",
        "test_size = 50\n",
        "\n",
        "train_set, test_set = torch.utils.data.random_split(data2, [train_size, test_size])\n",
        "\n",
        "## TODO: check training set and test set size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o35Rx_TnrvjX"
      },
      "source": [
        "## 2. The AlexNet model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXYNtiCEtpo1"
      },
      "source": [
        "### 2a. Download the net, not pre-trained\n",
        "Go to the [AlexNet page](https://pytorch.org/hub/pytorch_vision_alexnet/) on the PyTorch website and load the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDVfCIqgtwiz"
      },
      "source": [
        "## TODO: download the AlexNet network, not pre-trained\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bprm4WRQtxJy"
      },
      "source": [
        "#### Show in your report a table with its structure\n",
        "Visualize the network structure. Use the `torch-inspect` library ([PyPI](https://pypi.org/project/torch-inspect/), [GitHub](https://github.com/jettify/pytorch-inspect)) to visualize the network structure in more detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnsv77Tpt32Q"
      },
      "source": [
        "## TODO: visualize the network structure\n",
        "# use both 'model.eval()' and 'summary()' from the 'torch-inspect' library\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4VuELKWrz9B"
      },
      "source": [
        "## 3. Training the Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uUjDA5yHJVn"
      },
      "source": [
        "### 3a. Design the architecture\n",
        "We modify some of the AlexNet layers to better suit our dataset. Consult the [documentation](https://pytorch.org/vision/stable/_modules/torchvision/models/alexnet.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPLZPIbmHNxq"
      },
      "source": [
        "## TODO: modify the 2 layers following these specifications:\n",
        "\n",
        "# first convolutional layer\n",
        "#   * set kernel size to 7\n",
        "#   * set stride to 2\n",
        "#   * set padding to 3 \n",
        "model.features[0] = nn.Conv2d(#...)\n",
        "\n",
        "# last linear layer (classifier)\n",
        "#   * set output shape to 5 (there are 5 categories) \n",
        "#...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUBvS-Jq_fsj"
      },
      "source": [
        "Visualize the modified architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBLhi_b6r2Y3"
      },
      "source": [
        "## TODO: visualize the modified architecture\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BL6bzzc8L5g"
      },
      "source": [
        "### 3b. Define the batch size and load the training and test set\n",
        "We define a batch size of 4, and load the training set and test set with `DataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYlnSK7VIbHH"
      },
      "source": [
        "## TODO:\n",
        "#   * define the 'BATCH_SIZE';\n",
        "#   * load the training set and the test set in 'train_loader'\n",
        "#     and 'test_loader'.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY5ho_TQAhs7"
      },
      "source": [
        "### 3c. Define a training function\n",
        "Define a training function to loop over the epochs. First, specify the number of epochs (80) and the learning rate (0.01). Then, use the Stochastic Gradient Descent (SGD) as the optimization algorithm and the `CrossEntropyLoss` as thr performance estimate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCSc5M2TuCEE"
      },
      "source": [
        "## TODO:\n",
        "#   * define 'NUM_EPOCHS' and 'LEARNING_RATE';\n",
        "#   * define 'optimizer' (SGD) and 'error' (CrossEntropyLoss);\n",
        "#   * complete the 'train' function. This function skeleton misses the\n",
        "#     code to calculate and register the training accuracy. You can add\n",
        "#     it to plot the accuracy curve later.\n",
        "\n",
        "#...\n",
        "\n",
        "# the training function\n",
        "def train(model, train_loader, NUM_EPOCHS):\n",
        "    train_loss = [] #list to memorize the loss values\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS): #loop over the epochs  \n",
        "        running_loss = 0.0\n",
        "        \n",
        "        for images, labels in train_loader:\n",
        "            # get the inputs and load them to the computation device \n",
        "            #...\n",
        "\n",
        "            # set the gradients to zero\n",
        "            #...\n",
        "\n",
        "            # feedforward pass\n",
        "            #...\n",
        "\n",
        "            # backpropagation\n",
        "            #...\n",
        "\n",
        "            # update the parameters\n",
        "            #...\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # calculate the loss value and add it to the list    \n",
        "        loss = running_loss / len(train_loader)\n",
        "        train_loss.append(loss)\n",
        "        # keep track of the training process\n",
        "        print('Epoch {} of {}, Train Loss: {:.4f}'.format(epoch+1, NUM_EPOCHS, loss))\n",
        "\n",
        "    return train_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPTS3Fb-D565"
      },
      "source": [
        "### 3d. Run the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quq9sWWxOb3v"
      },
      "source": [
        "# set the model in train mode\n",
        "model.train()\n",
        "\n",
        "## TODO:\n",
        "#   * load the model to the computation device;\n",
        "#   * execute the training function.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NJOjxpfD-nk"
      },
      "source": [
        "### 3e. Save the trained network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1rr5w__Ayqb",
        "outputId": "7a0112d7-44a0-44da-c060-bd93039170af"
      },
      "source": [
        "# save the model\n",
        "print(\"Saving the model...\")\n",
        "\n",
        "try:\n",
        "    torch.save(model.state_dict(), \"AlexNet_flowers_saved_network.pth\")\n",
        "    print(\"Model saved!\")\n",
        "except:\n",
        "    print(\"Could not save the model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving the model...\n",
            "Model saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m5xiuSluCt6"
      },
      "source": [
        "## 4. Test and performance evaluations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM774JTOOVSs"
      },
      "source": [
        "# set the model in evaluation mode\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmtXvn-nDt76"
      },
      "source": [
        "### 4a. Define and execute a test function \n",
        "Define a test function that calculates the accuracy of the trained network by giving it as inputs the test set. While doing so, print the first 2 batches of images of the test set; check whether their class prediction is correct; print the predicted class as compared the right one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JoVJy_AuFRa"
      },
      "source": [
        "## TODO: complete the 'test' function\n",
        "# hint: it's similar to the testing part in the trainin loop in Project 1\n",
        "\n",
        "def test(model, test_loader):\n",
        "    predictions_list = [] \n",
        "    total = 0\n",
        "    correct = 0\n",
        "    batch = 1 #batch counter\n",
        "    \n",
        "    for images, labels in test_loader:\n",
        "        # get the inputs and load them to the computation device \n",
        "        #...\n",
        "\n",
        "        # prediction on the trained network\n",
        "        outputs = #... \n",
        "\n",
        "        # get the predicted class\n",
        "        predictions = #... \n",
        "        # append the predicted class to 'predictions_list'\n",
        "        #...\n",
        "        correct += (predictions == labels).sum() #increase the correct predictions counter\n",
        "                                                 #when the prediction matches the correct label\n",
        "        total += len(labels) \n",
        "\n",
        "        if batch <= 2: #print the images only for the first 2 batches\n",
        "            figure = plt.figure(figsize=(15, 20))\n",
        "            cols, rows = BATCH_SIZE, 1\n",
        "            for j in range(cols):\n",
        "                # visualize the images of the batch\n",
        "                figure.add_subplot(rows, cols, j+1)\n",
        "                if labels[j] == predictions_list[batch-1][j]: #for each image, check if\n",
        "                    pred = \"CORRECT\"                          #it was correctly predicted\n",
        "                else:\n",
        "                    pred = \"WRONG\"\n",
        "                plt.title(\"Correct class: {}\\nPredicted class: {}\\nPrediction: {}\". format(labels_dict[labels[j].item()], \n",
        "                                                                                           labels_dict[predictions_list[batch-1][j].item()],\n",
        "                                                                                           pred))\n",
        "                plt.imshow(np.clip(images[j].permute(1,2,0), 0., 1.))\n",
        "        batch += 1\n",
        "\n",
        "    # calculate the accuracy\n",
        "    accuracy = #...\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZash6_tHHhu"
      },
      "source": [
        "#### Execute the test function and print the accuracy value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqAJ0iV2t7ts"
      },
      "source": [
        "## TODO: execute the test function and print the \n",
        "# accuracy value with 2 decimal places\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9IUcWr_uF8H"
      },
      "source": [
        "### 4b. Performance curves\n",
        "Plot the loss and accuracy curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPgyIHb2uIo7"
      },
      "source": [
        "## TODO:\n",
        "#   * plot the loss curve;\n",
        "#   * plot the accuracy curve (remember you have to modify\n",
        "#     and add the code to calculate it in the 'train' function\n",
        "#     if you haven't already done so)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UMw01KEOfiH"
      },
      "source": [
        "## 5. Extracting Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VbOzf-nOjSQ"
      },
      "source": [
        "### 5a. Load the saved network and set it to evaluation mode\n",
        "\n",
        "Read about saving and loading models in the [documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
        "\n",
        "It's more convenient to save the model using `state_dict` (see section **3e. Save the trained network**):\n",
        "```\n",
        "torch.save(model.state_dict(), PATH)\n",
        "```\n",
        "\n",
        "Steps to take to load a model, especially if you open this notebook and it's not already connected to a runtime (namely, you don't have any executed code, so no defined variables, etc):\n",
        "* upload to Colab the `AlexNet_flowers_saved_network.pth` file using the file browser on the left side panel (select `Files` tab > click on `Upload to session storage`);\n",
        "* run all the code blocks BEFORE the training. This will re-define the model structure, which is needed to load the model;\n",
        "* use `model.load_state_dict(torch.load(PATH))` to load the model (remember to read the [documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html)).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HyiTuqeOv7H"
      },
      "source": [
        "## TODO: load the model and set it to evaluation mode (model.eval()).\n",
        "# IMPORTANT: read the instructions above this code cell.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epk72shKTvYn"
      },
      "source": [
        "### 5b. Visualize the learned kernels of the first convolutional layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3CaaXirT27v"
      },
      "source": [
        "#### Extract the convolutional layers from the model\n",
        "\n",
        "Create a list called `layers_list` to save the desired layers. Add to this list the first 2 convolutional layers. \n",
        "\n",
        "Then, save into a variable (`first_conv_layer_filters`) the weights of the filters of the first convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq4Wms5RT_ZQ"
      },
      "source": [
        "# get all the model children as list\n",
        "model_children = list(model.children())\n",
        "\n",
        "layers_list = [] # list to save the layers in\n",
        "\n",
        "## TODO: add to the list the desired layers \n",
        "# hint: print model_children to examine it, and extract the layers from here\n",
        "layers_list.extend([\n",
        "                    #...\n",
        "                  ]) \n",
        "\n",
        "# weights of the first convolutional layer\n",
        "first_conv_layer_filters = model_children[0][0].weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVRSWx9VUYm6"
      },
      "source": [
        "Print the layers in `layers_list` to check them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNE896hGUcqd"
      },
      "source": [
        "## TODO: print the layers in layers_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBYECYytUjMj"
      },
      "source": [
        "#### Visualize the first convolutional layer filters\n",
        "\n",
        "Visualize 16 of the first convolutional layer filters in a 4x4 grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fT3YydnUl9v"
      },
      "source": [
        "# visualize 16 of the the first convolutional layer kernels\n",
        "plt.figure(figsize=(8, 8)) #set the width and height of the figure\n",
        "for i, filter in enumerate(first_conv_layer_filters): #loop over the kernels in the first conv layer\n",
        "    if i < 16: #we want to visualize only 16 kernels\n",
        "        # plot a filter\n",
        "        plt.subplot(4, 4, i+1) \n",
        "        plt.imshow(filter[0, :, :].detach().cpu(), cmap='gray')\n",
        "        plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR0LiJCUU2Q_"
      },
      "source": [
        "### 5c. Visualize the feature maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBmIdzinU5zu"
      },
      "source": [
        "#### Get an image from the test set\n",
        "Get a random image from the test set and visualize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1L5U9V3U8h-"
      },
      "source": [
        "## TODO: \n",
        "# * get an image and its label from the test set\n",
        "# * print the image size (it should be [1, 3, 64, 64])\n",
        "# * visualize the image\n",
        "\n",
        "# the image can be randomly selected, but it's better if the flower is\n",
        "# clearly visible and occupies a lot of space. So if you select an \n",
        "# image at random, run the code until a good image is selected.\n",
        "#\n",
        "# Name the image as 'img' (we will use it later)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZvwzk93Vtc5"
      },
      "source": [
        "#### Pass the input image through each convolutional layer\n",
        "Pass the image through the layers in `layers_list`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2IszzALV31D"
      },
      "source": [
        "# TODO: pass the image through the convolutional layers\n",
        "pass_first_conv = layers_list[0](img) #image passed through the first conv layer\n",
        "pass_second_conv = #... #output of first conv layer passed\n",
        "                        #through the second conv layer\n",
        "\n",
        "outputs = [pass_first_conv, pass_second_conv]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLITsojdZ-uj"
      },
      "source": [
        "#### Visualize the feature maps from the 1st and 2nd convolutional layer\n",
        "\n",
        "Visualize 16 feature maps from the first and the second convolutional layer. Note that each of the layers has more than 16 feature maps, but we want to show only 16 to visualize them better. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjePMe7eaeo-"
      },
      "source": [
        "## TODO: visualize 16 feature maps (filters) from the 1st convolutional layer \n",
        "#        and 16 feature maps from the 2nd convolutional layer, each of them in \n",
        "#        a 4x4 grid.\n",
        "# hints:\n",
        "# * The code to get the 64 feature maps of the 1st conv layer is the following:\n",
        "#       feature_maps = outputs[0][0, :, :, :]\n",
        "#       feature_maps = feature_maps.data\n",
        "#   (print its size to inspect it). Remember you have to visualize the first 16 \n",
        "#   out of 64. \n",
        "# * To tile the images (e.g. no white space between them) use\n",
        "#       plt.subplots_adjust(hspace=-.02, wspace=-.02)\n",
        "#   right before plt.show().\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrhqxMTF-r68"
      },
      "source": [
        "## Results, Observations and Conclusions\n",
        "TODO: Write your own notes, observations and conclusions about the results of your work.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HtPNgw3-yn_"
      },
      "source": [
        "## Full Code\n",
        "TODO: Report the complete code of the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTPCa7u_r_EK"
      },
      "source": [
        "---\n",
        "## Bibliography\n",
        "\n",
        "* [Visualizing Filters and Feature Maps in Convolutional Neural Networks using PyTorch](https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/)"
      ]
    }
  ]
}