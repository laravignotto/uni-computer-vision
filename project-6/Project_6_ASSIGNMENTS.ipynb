{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_6_ASSIGNMENTS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvPdg3g8YCc-"
      },
      "source": [
        "# Project 6: Object Recognition\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHh0mY6XYTCT"
      },
      "source": [
        "## Assignments\n",
        "\n",
        "Please, edit your report by fulfilling the following list of assignments.\n",
        "\n",
        "**Introduction.** Short summary of the goals of the  project. The sections composing the report.\n",
        "\n",
        "**Section 1. Data loading and visualization**\n",
        "* 1a. Download the data and visualize an image from the dataset;\n",
        "* 1b. Write a custom dataset for Penn-Fudan;\n",
        "* 1c. Visualize images with ground truth.\n",
        "\n",
        "**Section 2. Data preparation**\n",
        "* 2a. Define a tronsformer function;\n",
        "* 2b. Define the training set and test set.\n",
        "\n",
        "**Section 3. Preparation of the training**\n",
        "* 3a. The NN model: download and adjust;\n",
        "* 3b. Define hyperparameters.\n",
        "\n",
        "**Section 4. Training execution**\n",
        "* 4a. Training;\n",
        "* 4b. Retrieve the loss data during training.\n",
        "\n",
        "**Section 5. Test**\n",
        "* 5a. Plot training losses;\n",
        "* 5b. Visualize the predicted bounding boxes.\n",
        "\n",
        "**Section 6. Results, observations and conclusions**\n",
        "\n",
        "**Full code**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8QIhbO1ZDAw"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZsKgt_cZZkD"
      },
      "source": [
        "## 1. Data loading and preparation\n",
        "\n",
        "Install the `pycocotools` library. It will be used for computing the evaluation metrics (accuracy) using the COCO metric for intersection over union. Then, import the libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBIoe_tHTQgV"
      },
      "source": [
        "!pip install cython\n",
        "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0OiManlaFtL"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.9.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXsXx2pnR--O"
      },
      "source": [
        "# imports\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torchvision.datasets.utils import download_and_extract_archive\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnPifXILvI2R"
      },
      "source": [
        "### 1a. Download the data and visualize an image from the dataset\n",
        "In this project we will use the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csYRUwQoDEre"
      },
      "source": [
        "#### Download and extract the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgLC8wH40wnl"
      },
      "source": [
        "## TODO: download the dataset and extract it in the current directory.\n",
        "#        The archive is located here:\n",
        "#             https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
        "# hints: * use 'download_and_extract_archive(url, root)' (see Project 4);\n",
        "#        * to specify the current directory use 'root = \".\"'.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo8P_ENLDHtr"
      },
      "source": [
        "#### Visualize an image and its corresponding segmentation mask\n",
        "\n",
        "Using PIL, open an image from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwHkYrqlDhYn"
      },
      "source": [
        "## TODO: open the PennFudanPed/PNGImages/ folder, look at the name\n",
        "#        of the images and change the 'image_path' string to visualize \n",
        "#        other images from the dataset.\n",
        "\n",
        "image_path = 'PennFudanPed/PNGImages/FudanPed00007.png'\n",
        "Image.open(image_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFB1e80Y0C8q"
      },
      "source": [
        "### 1b. Write a custom dataset for Penn-Fudan\n",
        "\n",
        "Write a custom class that inherits from `torch.utils.data.Dataset` for defining the Penn-Fudan dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4oZatGlEB1l"
      },
      "source": [
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhLX1ntxEIbw"
      },
      "source": [
        "Let's see how the outputs are structured for this dataset by instantiating a `PennFudanDataset` object and printing the first element of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBasq8wUEden"
      },
      "source": [
        "## TODO: instantiate a PennFudanDataset object, call it 'dataset'.\n",
        "#        Then, print the length of the dataset and an element from the dataset.\n",
        "# hints: * The PennFudanDataset class accepts a path as argument, in this \n",
        "#          case we have to give it the path to the extracted folder, which\n",
        "#          is 'PennFudanPed/';\n",
        "#        * To print an element, call the 'dataset' object giving it an index\n",
        "#          (it can be from 0 to the length of the dataset minus 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSkUPxegMx9E"
      },
      "source": [
        "### 1c. Visualize images with ground truth\n",
        "\n",
        "Define three functions: \n",
        "* `show_boxed_image` draws bounding boxes around pedestrians in an image;\n",
        "* `show_area_image` draws area rectangles above pedestrians;\n",
        "* `show_mask_image` constructs a tensor with all the segmentation masks of pedestrians in the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P72Cuqea5Ec_"
      },
      "source": [
        "def show_boxed_image(img):\n",
        "    '''\n",
        "    args: img -> PennFudanDataset object\n",
        "    returns an ImageDraw image with bounding boxes around pedestrians.\n",
        "    '''\n",
        "    img_show = img[0] #get the PIL image\n",
        "    img_coordinates = img[1].get(\"boxes\") #get the bounding boxes coordinates\n",
        "\n",
        "    # draw a rectangle (bounding box) following the coordinates\n",
        "    for coords in img_coordinates:\n",
        "        ## TODO: get the coordinates\n",
        "        xmin = coords[0]\n",
        "        ymin = #...\n",
        "        xmax = #...\n",
        "        ymax = #...\n",
        "        draw = ImageDraw.Draw(img_show)\n",
        "        # draw the rectangle\n",
        "        # (xmin, ymin) -> upper left corner\n",
        "        # (xmax, ymax) -> lower right corner\n",
        "        draw.rectangle([(xmin, ymin), (xmax, ymax)], \n",
        "                       outline =\"lightgreen\", \n",
        "                       width=5)\n",
        "\n",
        "    return img_show\n",
        "\n",
        "\n",
        "def show_area_image(img):\n",
        "    '''\n",
        "    args: img -> PennFudanDataset object\n",
        "    returns an ImageDraw image with area rectangles over pedestrians.\n",
        "    '''\n",
        "    ## TODO:\n",
        "    # get the PIL image\n",
        "\n",
        "    # get the bounding boxes coordinates\n",
        "\n",
        "    # get the area values\n",
        "    img_areas = #...\n",
        "\n",
        "    fnt = ImageFont.truetype(\"~/usr/share/fonts/truetype/LiberationMono-Bold.ttf\", 20)\n",
        "\n",
        "    # draw a filled rectangle following the coordinates, and \n",
        "    # print the value of the area over each rectangle\n",
        "    for i in range(len(img_areas)):\n",
        "        ## TODO: get the coordinates\n",
        "        # ATTENTION: you have to retrieve them in a different way in respect\n",
        "        #            to the previous function (look at the for loop...)\n",
        "        xmin = #...\n",
        "        #...\n",
        "\n",
        "        ## TODO: draw a filled rectangle using the coordinates.\n",
        "        #...\n",
        "\n",
        "        # Draw the corresponding area over each rectangle\n",
        "        draw.text(((xmax+xmin)/2-40, (ymin+ymax)/2), \n",
        "                  str(img_areas[i].item()), \n",
        "                  font = fnt,\n",
        "                  fill=\"blue\")\n",
        "\n",
        "    return img_show\n",
        "\n",
        "\n",
        "def show_mask_image(img):\n",
        "    '''\n",
        "    args: img -> PennFudanDataset object\n",
        "    returns a tensor corresponding to the segmentation masks of pedestrians \n",
        "    in the image.\n",
        "    '''\n",
        "    ## TODO: get the masks tensors\n",
        "    masks = #...\n",
        "\n",
        "    # add each tensor (mask) onto the others. \n",
        "    # The result is a single tensor containing all the segmentation masks\n",
        "    result = torch.zeros(masks.size()[1], masks.size()[2])\n",
        "    for i in range(len(masks)):\n",
        "        result += masks[i]\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-QBQ_1lXoRW"
      },
      "source": [
        "Create a list of four random integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viUuvn94M8O9"
      },
      "source": [
        "## TODO: create a list containing 4 random integers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYOtqS9kYZKe"
      },
      "source": [
        "Visualize 16 images:\n",
        "* 1st row: 4 random images from the dataset (use the list of 4 random numbers);\n",
        "* 2nd row: the same 4 images, with bounding boxes;\n",
        "* 3rd row: the same 4 images, with areas;\n",
        "* 4th row: the segmentation masks of the same 4 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNZOC6RUzbWl"
      },
      "source": [
        "## TODO: visualize 4 original images.\n",
        "\n",
        "\n",
        "## TODO: visualize 4 images with bounding boxes.\n",
        "\n",
        "\n",
        "## TODO: visualize 4 images with areas.\n",
        "\n",
        "\n",
        "## TODO: visualize 4 images with segmentation masks.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnaeVhr6aKOj"
      },
      "source": [
        "## 2. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvLFQh9Pa0DO"
      },
      "source": [
        "### 2a. Define a tronsformer function\n",
        "\n",
        "Write a function that enhances the training set by doing data transformation and augmentation, namely horizontal flip of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MGJ-gGFbGhh"
      },
      "source": [
        "def get_transform(train):\n",
        "    transforms_to_apply = []\n",
        "    ## TODO: convert the PIL image into a PyTorch Tensor\n",
        "    transforms_to_apply.append(\n",
        "        transforms.#...\n",
        "        )\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        transforms_to_apply.append(transforms.RandomHorizontalFlip(0.5))\n",
        "    return transform.Compose(transforms_to_apply)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6jAZKHMbm_K"
      },
      "source": [
        "### 2b. Define the training set and test set\n",
        "\n",
        "Instantiate the training and testing data classes and assign them to `DataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88XwrMRVbnZY"
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "train_set = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "test_set = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(train_set)).tolist()\n",
        "## TODO: using torch.utils.data.Subset, define the train_set and test_set.\n",
        "# hints: * train_set will be a new subset of train_set defined above. The\n",
        "#          same applies to the test_set;\n",
        "#        * get the indices from the 'indices' variable defined above. The\n",
        "#          train_set must contain 100 elements (e.g. the first 100 indices),\n",
        "#          and the test_set must contain 20 elements (e.g. the indices from\n",
        "#          100 to 119). Pay attention to how the indices are selected in \n",
        "#          list slicing.\n",
        "\n",
        "\n",
        "## TODO: define training and test data loaders. Use 'DataLoader'.\n",
        "#        Some arguments:\n",
        "#        train_loader: batch size=2, shuffle=True, num_workers=2, \n",
        "#                      collate_fn=utils.collate_fn\n",
        "#        test_loader: batch size=1, shuffle=False, num_workers=2, \n",
        "#                     collate_fn=utils.collate_fn\n",
        "\n",
        "\n",
        "## TODO: check the size of the training set and the test set by printing it\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siXojekX2yvq"
      },
      "source": [
        "## 3. Preparation of the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY94_nEC24A7"
      },
      "source": [
        "### 3a. The NN model: download and adjust\n",
        "\n",
        "Define a `get_model` function to download a pre-trained model. Adjust its input layer to resize the images, and the last layer to output a number of classes that suits our dataset.\n",
        "\n",
        "We will use a Faster R-CNN model with a Resnet-50 backbone.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPnLayQo1msq"
      },
      "source": [
        "def get_model(num_classes):\n",
        "    ## TODO: load an object detection model pre-trained on COCO.\n",
        "    #        Construct a Faster R-CNN model with a ResNet-50-FPN backbone by\n",
        "    #        using torchvision.models.detection.fasterrcnn_resnet50_fpn().\n",
        "    #        We want the model to be pre-trained.\n",
        "    #        https://pytorch.org/vision/stable/models.html#torchvision.models.detection.fasterrcnn_resnet50_fpn\n",
        "    model = #...\n",
        "\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    \n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "   \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHTBagjU4IPw"
      },
      "source": [
        "Instantiate the model and apply a resize on the images. The input images resolution must be 224x224 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO7ByZc4q2uJ"
      },
      "source": [
        "## TODO: if available use GPU, else use CPU.\n",
        "\n",
        "\n",
        "## TODO: define NUM_CLASSES (we only have 2: background and person).\n",
        "\n",
        "\n",
        "## TODO: get the model using the 'get_model' function.\n",
        "model = #...\n",
        "\n",
        "# Perform input / target transformation before feeding the data to the model\n",
        "grcnn = torchvision.models.detection.transform.GeneralizedRCNNTransform(min_size=224, \n",
        "                                                                        max_size=224, \n",
        "                                                                        image_mean=[0.485, 0.456, 0.406], \n",
        "                                                                        image_std=[0.229, 0.224, 0.225]\n",
        "                                                                        )\n",
        "model.transform = grcnn #apply the transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0ntlETn3ls6"
      },
      "source": [
        "### 3b. Define hyperparameters\n",
        "\n",
        "Implement a Stochastic Gradient Descent optimizer with a learning rate of 0.0005, a momentum factor of 0.9, and a weight decay of 0.0005. Then, construct a learning rate scheduler which decreases the learning rate by 10 times every 3 epochs. Set the number of epochs at 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK_mEnFZ3Tew"
      },
      "source": [
        "# construct the optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "## TODO: complete the arguments.\n",
        "#        https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
        "#        learning rate of 0.0005, momentum factor of 0.9, \n",
        "#        weight decay of 0.0005.\n",
        "optimizer = torch.optim.SGD(params, \n",
        "                           #...\n",
        "                           )\n",
        "\n",
        "# construct the learning rate scheduler\n",
        "## TODO: complete the arguments.\n",
        "#        https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR\n",
        "#        step size of 3, gamma of 0.1.\n",
        "# parameters: step_size: period of learning rate decay;\n",
        "#             gamma: multiplicative factor of learning rate decay.\n",
        "# The StepLR decays the learning rate of each parameter group by gamma every step_size epochs.\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "                                              #...    \n",
        "                                              )\n",
        "\n",
        "## TODO: define the number of epochs (10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxmkmJHQsNCf"
      },
      "source": [
        "## 4. Training execution\n",
        "\n",
        "At the beginning of the notebook, we copied some helper functions to simplify training and evaluating models in `references/detection/`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lZmVgfPih_s"
      },
      "source": [
        "### 4a. Training\n",
        "\n",
        "Train the model for 10 epochs, evaluating at the end of every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAXjONrU357_"
      },
      "source": [
        "## TODO: move model to the right device\n",
        "\n",
        "\n",
        "metric_collector = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    ## TODO: using the 'train_one_epoch' function from 'engine', train for\n",
        "    #        one epoch, printing every 50 iterations (print_freq=50).\n",
        "    # hints: you can find the source code here \n",
        "    #        https://github.com/pytorch/vision/blob/master/references/detection/engine.py\n",
        "    #        these are the arguments:\n",
        "    #        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq)\n",
        "    metric_logger = train_one_epoch(\n",
        "                                   #...\n",
        "                                   )\n",
        "    metric_collector.append(metric_logger)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R95ccLhML404"
      },
      "source": [
        "### 4b. Retrieve the loss data during training\n",
        "\n",
        "The task of object detection has more than one loss value, so during training these are the losses calculated at every epoch:\n",
        "* `loss`: the sum of all losses;\n",
        "* `loss_classifier`: measures the performance of the object classification for detected bounding boxes;\n",
        "* `loss_box_reg`: measures the performance of the network for retrieving the coordinates of the ground truth bounding boxes;\n",
        "* `loss_objectness`: measures the performance of the network for retrieving bounding boxes which contain an object; \n",
        "* `loss_rpn_box_reg`: measures the performance of the network for retrieving the region proposals.\n",
        "\n",
        "Retrieve the `loss` and `loss_box_reg` metrics from the `metric_collector` and save them in a `metrics` list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es5dao-yHqZ0"
      },
      "source": [
        "loss_metric = []\n",
        "loss_classifier_metric = []\n",
        "loss_box_reg_metric = []\n",
        "loss_objectness_metric = []\n",
        "loss_rpn_box_reg_metric = []\n",
        "\n",
        "# str. split([sep[, maxsplit]])\n",
        "# maxsplit: number of splits to do; default is -1 which splits all the items.\n",
        "# since we want just one split (e.g.: '0.8763' and '(0.8763)') we can write '.split(' ', 1)'.\n",
        "\n",
        "for i in range(len(metric_collector)):\n",
        "    # get the loss value\n",
        "    loss = float(str(metric_collector[i].__getattr__(\"loss\")).split(' ', 1)[0])\n",
        "    # add the loss value to the 'loss_metric' list\n",
        "    loss_metric.append(loss)\n",
        "\n",
        "    ## TODO: get the loss_classifier, loss_box_reg, loss_objectness and\n",
        "    #        loss_rpn_box_reg, and add them to their lists.\n",
        "    \n",
        "\n",
        "# list of lists of all losses\n",
        "metrics = [loss_metric, \n",
        "           loss_classifier_metric, \n",
        "           loss_box_reg_metric, \n",
        "           loss_objectness_metric, \n",
        "           loss_rpn_box_reg_metric]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idCjorepTW51"
      },
      "source": [
        "Save the metrics and the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN0fT-VY4Wwl"
      },
      "source": [
        "# save the metrics (list) to a pickle file\n",
        "with open('metrics.pickle', 'wb') as f:\n",
        "    pickle.dump(metrics, f)\n",
        "\n",
        "## TODO: save the model using 'state_dict()'.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e9irp3F203M"
      },
      "source": [
        "## 5. Test\n",
        "\n",
        "Load the metrics and the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swdx558Z4VlS"
      },
      "source": [
        "# load the list of metrics\n",
        "with open('metrics.pickle', 'rb') as f:\n",
        "    metrics = pickle.load(f)\n",
        "\n",
        "# load the model\n",
        "loaded_model = get_model(num_classes = 2)\n",
        "## TODO: * load the model using 'load_state_dict()';\n",
        "#        * put the model in evaluation mode.\n",
        "loaded_model.#...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_0Stv3GOmdV"
      },
      "source": [
        "### 5a. Plot training losses\n",
        "\n",
        "Plot the data of the losses during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz3_didcKgmK"
      },
      "source": [
        "## TODO: plot the 5 curves of the losses (loss, loss_classifier, loss_box_reg, \n",
        "#        loss_objectness and loss_rpn_box_reg) in a 2x3 grid.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RzrfogT4BLk"
      },
      "source": [
        "### 5b. Visualize the predicted bounding boxes\n",
        "\n",
        "Give 8 images to the model and visualize the predicted bounding boxes.\n",
        "* The last 4 images (first row) must show the ground truth bounding boxes and ALL the predicted bounding boxes;\n",
        "* The first 4 images (second row) must show the ground truth bounding boxes and the predicted bounding boxes with a score greater than 0.8.\n",
        "\n",
        "The following code selects the first image from the test set and shows it with the ground truth bounding boxes and the predicted bounding boxes with a score greater than 0.8. Complete it and then modify it following the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d_lKThsK0aZ"
      },
      "source": [
        "# get an image and its bounding box from the test set\n",
        "idx = 0 #index of the first image of the test set\n",
        "img, _ = test_set[idx]\n",
        "bounding_boxes = test_set[idx][1][\"boxes\"] #ground truth\n",
        "\n",
        "# get predictions\n",
        "prediction = loaded_model([img]) #predicted bounding boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE6WLI9GVk_i"
      },
      "source": [
        "## TODO: print 'bounding_boxes' and 'prediction' to analyze them and see\n",
        "#        how they look like.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2iNjtxzLjF0"
      },
      "source": [
        "# transform the image from tensor to PIL\n",
        "img_PIL = torchvision.transforms.ToPILImage()(img.squeeze(0))\n",
        "\n",
        "# draw image\n",
        "draw = ImageDraw.Draw(img_PIL)\n",
        "\n",
        "# draw ground truth\n",
        "for box in range(len(bounding_boxes)): #for each bounding box in the ground truth\n",
        "    # draw it on the image\n",
        "    draw.rectangle([\n",
        "                   ## TODO: coordinates of the bounding box \n",
        "                   (bounding_boxes[box][0], #...\n",
        "                   ], \n",
        "                   ## TODO: color of the outline (green) and width\n",
        "                   )\n",
        "\n",
        "# draw prediction\n",
        "predicted_boxes = prediction[0][\"boxes\"]\n",
        "predicted_scores = prediction[0][\"scores\"]\n",
        "\n",
        "for box in range(len(predicted_boxes)):\n",
        "    # get the predicted bounding boxes\n",
        "    boxes = predicted_boxes[box].detach().numpy()\n",
        "    # get the scores (percentage) of the predicted bounding boxes\n",
        "    score = np.round(predicted_scores[box].detach().numpy(), decimals= 4)\n",
        "    if score > 0.8: #draw only high scoring boxes\n",
        "        ## TODO: draw a red rectangle for the predicted bounding box\n",
        "        #...\n",
        "        # write the score of each predicted bounding box in the top left corner\n",
        "        draw.text((boxes[0], boxes[1]), text = str(score))\n",
        "\n",
        "img_PIL #show the image with the bounding boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU_Lg2wBWJ8c"
      },
      "source": [
        "## TODO: After completing the previous code, modify it following the previous\n",
        "#        instructions (show 8 images in 2 rows of 4 images each)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3pCTulaF3v"
      },
      "source": [
        "## 6. Results, observations and conclusions\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyhVrmhIaIGp"
      },
      "source": [
        "## Full code\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b95oAaOGOblH"
      },
      "source": [
        "---\n",
        "\n",
        "## Bibliography\n",
        "* [Torchvision Object Detection Finetuning Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
        "* [Building your own object detector — PyTorch vs TensorFlow and how to even get started?](https://towardsdatascience.com/building-your-own-object-detector-pytorch-vs-tensorflow-and-how-to-even-get-started-1d314691d4ae)"
      ]
    }
  ]
}